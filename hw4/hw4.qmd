---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 24 @ 11:59PM
author: Jonathan Hori - 305947261
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
---

Display machine information:
```{r}
#| eval: true

sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
#| eval: true
# install.packages("bonsai")
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(gtsummary))
# library(keras)
# install_keras()
library(bonsai)
library(lightgbm)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(lubridate))
```

# Predicting 30-day mortality

Using the ICU cohort `icu_cohort.rds` you built in Homework 3, develop at least three analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression with elastic net (lasso + ridge) penalty (e.g., glmnet or keras package), (2) random forest, (3) boosting, and (4) support vector machines, or (5) MLP neural network (keras package)

1. Partition data into 50% training set and 50% test set. Stratify partitioning according the 30-day mortality status.

2. Train and tune the models using the training set.

3. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each model.

# Solution
## Load and setup data
```{r}
icu_data_path <- "./icu_cohort.rds"
cohort_data <- read_rds(icu_data_path)

cohort_data %>% print(width = Inf)
```

Drop all id columns and variables related to discharge.

```{r}
cohort_cleaned <- cohort_data %>% 
  select(-subject_id,
         -hadm_id,
         -stay_id,
         -intime,
         -last_careunit,
         -outtime,
         -los,
         -dischtime,
         -deathtime,
         -discharge_location,
         -edregtime,
         -edouttime,
         -hospital_expire_flag,
         -anchor_age,
         -anchor_year,
         -anchor_year_group,
         -dod) %>% 
  mutate(
    admit_hour = hour(admittime),
    thirty_day_mort = as.factor(thirty_day_mort)
  ) %>% 
  select(
    -admittime
  ) %>% 
  print(width = Inf)

```

We choose to coalesce all categorical variables having levels with fewer than 
3% of that variable's total number of patients into "other" categories for these 
variables.
```{r}
lump_prop = 0.03

cohort_cleaned <- cohort_cleaned %>% 
  mutate(
    first_careunit = fct_lump_prop(first_careunit, lump_prop),
    admission_type = fct_lump_prop(admission_type, lump_prop),
    admission_location = fct_lump_prop(admission_location, lump_prop),
    ethnicity = fct_lump_prop(ethnicity, 0.028) 
    # I want to keep Asian separate since it's borderline and I'm curious. 
    #   I know this line is less reproducible. 
  )

cohort_cleaned %>% tbl_summary(thirty_day_mort)
```

### Train/test split

```{r}
set.seed(100)
data_split <- initial_split(
  cohort_cleaned,
  strata = thirty_day_mort,
  prop = 0.5
) 
data_split

cohort_train <- training(data_split)
dim(cohort_train)

cohort_test <- testing(data_split)
dim(cohort_test)
```


## Data processing recipes

I think lab and vital measurements will be important for predicting mortality,
and choose to impute missing values here using medians. I experiment with
alternate methods to handle outlier values. I consider as an outlier any value 
below the 5th percentile, or above the 95th percentile.

```{r}
remove_outliers <- function(col) {
  quantiles <- quantile(col, probs = c(0.5, 0.95), na.rm = TRUE)
  col[col < quantiles[1] | col > quantiles[2]] <- NA
  return(col)
}

truncate_outliers <- function(col) {
  quantiles <- quantile(col, probs = c(0.5, 0.95), na.rm = TRUE)
  col[col < quantiles[1]] <- quantiles[1]
  col[col > quantiles[2]] <- quantiles[2]
  return(col)
}

base_recipe <- recipe(
  thirty_day_mort ~ .,
  data = cohort_train
  )

base_with_outlier_filter <- base_recipe %>% 
  step_mutate_at(all_numeric_predictors(), -age, fn = remove_outliers)

base_with_outlier_truncation <- base_recipe %>% 
  step_mutate_at(all_numeric_predictors(), fn = truncate_outliers)
```


I'm not sure if there's a better way to reuse a set of multiple steps in more
than one recipe, so this section isn't very DRY. However, these steps shoudld 
come after outlier adjustment.
```{r}
recipe_with_filter_normalization_and_dummies <- 
  base_with_outlier_filter %>% 
  step_impute_median(all_numeric_predictors(), -age) %>% 
  step_unknown(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  prep(training = cohort_train, retain = TRUE)

recipe_with_truncation_normalization_and_dummies <- 
  base_with_outlier_truncation %>% 
  step_impute_median(all_numeric_predictors(), -age) %>% 
  step_unknown(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  prep(training = cohort_train, retain = TRUE)
  
recipe_with_normalization_and_dummies <- base_recipe %>% 
  step_impute_median(all_numeric_predictors(), -age) %>% 
  step_unknown(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  prep(training = cohort_train, retain = TRUE)


```

Our transformed data (without outlier transformation) looks like this: 
```{r}
baked_data <- bake(recipe_with_normalization_and_dummies, new_data = NULL)

baked_data %>% head() %>% print(width = Inf)
```


## Create prediction models with custom parameter grids

### Logistic Regression
```{r}
logistic_model <- logistic_reg(
    penalty = tune(),
    mixture = tune()
  ) %>% 
  set_engine("glmnet", standardize = FALSE)
logistic_model
```

```{r}
logistic_grid <- grid_regular(
  penalty(),
  mixture(),
  levels = c(100, 5)
)
logistic_grid %>% head
```

### Random forest
```{r}
rf_model <- rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_model
```

```{r}
rf_grid <- grid_regular(
  trees(range = c(100, 300)),
  mtry(range = c(1, 5)),
  levels = c(3, 5)
)
rf_grid %>% head
```


### Boosted decision trees

Using `lightgbm` for speed in training rather than `xgboost`.

```{r}
boosted_model <- boost_tree(
  mode = "classification",
  trees = 1000,
  tree_depth = tune(),
  learn_rate = tune()
) %>% 
  set_engine("lightgbm")
boosted_model
```
```{r}
boosted_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
)
boosted_grid %>% head
```


### Multilayer perceptron, 1 hidden layer

Note: Had bugs with keras on my M1 mac that I couldn't debug (possibly related
to `tensorflow-deps`?). Retaining model here as an example, but it is not used 
in the final workflowset.

```{r}
mlp_model <- mlp(
  mode = "classification",
  hidden_units = tune(),
  dropout = tune(),
  epochs = 20
) %>% 
  set_engine("keras", verbose = 0)
mlp_model
```

```{r}
mlp_grid <- grid_regular(
  hidden_units(range = c(1, 20)),
  dropout(range = c(0, 0.6)),
  levels = 5
)
mlp_grid %>% head
```


## Create workflowset with custom grids

We add a custom parameter grid for each model, which in a workflowset will 
correspond to unique `workflow_id`s. So we have to specify a grid for each 
recipe-model pair. 

This manual process of adding a custom grid for each pair leads me to think 
using workflowsets makes the most sense when comparing models
in the same family and having the same types of parameters.

```{r}
mimic_models <- 
  workflow_set(
    preproc = list(
      simple = recipe_with_normalization_and_dummies,
      filtering = recipe_with_filter_normalization_and_dummies,
      truncating = recipe_with_truncation_normalization_and_dummies
    ),
    models = list(
      logit = logistic_model,
      rf = rf_model,
      boosted = boosted_model
    ),
    cross = TRUE
  ) %>% 
  
  # Add a unique parameter grid for each model type. Not all parameters are
  #   applicable for every model. 
  option_add(grid = logistic_grid, id = "simple_logit") %>% 
  option_add(grid = logistic_grid, id = "filtering_logit") %>%
  option_add(grid = logistic_grid, id = "truncating_logit") %>% 
  option_add(grid = rf_grid, id = "simple_rf") %>%
  option_add(grid = rf_grid, id = "filtering_rf") %>%
  option_add(grid = rf_grid, id = "truncating_rf") %>%
  option_add(grid = boosted_grid, id = "simple_boosted") %>% 
  option_add(grid = boosted_grid, id = "filtering_boosted") %>%
  option_add(grid = boosted_grid, id = "truncating_boosted")
mimic_models
```



## Run cross-validation for all models in workflowset

Cross-validation splits. Split into five folds. 
```{r}
cv_split <- vfold_cv(cohort_train, v = 5)
```

Run cross-validation.
```{r}
st_time <- proc.time()

workflow_cv_fit <- mimic_models %>% 
  workflow_map(
    resamples = cv_split,
    metrics = metric_set(roc_auc, accuracy),
    verbose = TRUE
  )

end_time <- proc.time()
print(end_time - st_time)

workflow_cv_fit
```

#### Evaluate CV results

```{r}
autoplot(workflow_cv_fit, metric = "roc_auc")
```

We observe some AUCs are below 0.5. 

```{r}
rank_results(workflow_cv_fit, rank_metric = "roc_auc", select_best = TRUE)
```

Lightgbm performs the best on an AUC basis, with the random forest model closely 
behind. 

#### Hyperparameter performance per model

Plotting the results for the best version of each model. In this case it is the 
"simple" data recipe, without any outlier adjustment.

```{r}
# Logistic - penalty vs. mixture
workflow_cv_fit %>% 
  extract_workflow_set_result(id = "simple_logit") %>% 
  collect_metrics %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = penalty, y = mean, color = mixture)) +
  geom_point() +
  labs(
    x = "Penalty",
    y = "Cross-validation AUC",
    title = "Logistic regression hyperparameter tuning results"
  ) %>% 
  scale_x_log10
  
# Logistic - trees vs. mtry
workflow_cv_fit %>%
  extract_workflow_set_result(id = "simple_rf") %>%
  collect_metrics %>%
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = trees, y = mean, color = mtry)) +
  geom_point() +
  labs(
    x = "Number of trees",
    y = "Cross-validation AUC",
    title = "Random forest hyperparameter tuning results"
  ) %>%
  scale_x_log10


# Boosted trees - learning rate vs. tree depth
workflow_cv_fit %>% 
  extract_workflow_set_result(id = "simple_boosted") %>% 
  collect_metrics %>% 
  filter(.metric == "roc_auc")%>% 
  ggplot(aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  labs(
    x = "Learning Rate",
    y = "Cross-validation AUC",
    title = "LightGBM hyperparameter tuning results"
  ) %>% 
  scale_x_log10
```


## Finalize best model

What is the best workflow for each model type?
```{r}
best_workflows <- rank_results(workflow_cv_fit, 
                               rank_metric = "roc_auc") %>% 
  filter(.metric == "roc_auc") %>% 
  group_by(model) %>% 
  arrange(desc(mean)) %>% 
  slice_head(n = 1) %>% 
  print(width = Inf)
```

Let's finalize our models for each of these workflows. Note selecting the 
best workflow given a workflowset is an [open issue for tidymodels](https://github.com/tidymodels/workflowsets/issues/94).

```{r}
finalize_model_from_set <- function(fit_workflowset, 
                                    workflow_id,
                                    cross_val_split) {
  best.config <- fit_workflowset %>% 
    extract_workflow_set_result(workflow_id) %>% 
    select_best("roc_auc") # only considering AUC for this task
  
  best <- fit_workflowset %>% 
    extract_workflow(workflow_id) %>% 
    finalize_workflow(best.config) %>% 
    last_fit(cross_val_split)
  
  return(best)
}
```


Best logistic regression:
```{r}
best_logit <- finalize_model_from_set(workflow_cv_fit,
                                      "simple_logit",
                                      data_split)
best_logit %>% collect_metrics()
```


Best random forest:
```{r}
best_rf <- finalize_model_from_set(workflow_cv_fit,
                                   "simple_rf",
                                   data_split)
best_rf %>% collect_metrics()
```


Best lightgbm model:
```{r}
best_lightgbm <- finalize_model_from_set(workflow_cv_fit,
                                         "simple_boosted",
                                         data_split)
best_lightgbm %>% collect_metrics()
```

As in cross-validation, we see the lightgbm model has the highest AUC at 0.84, 
with random forest behind at 0.84, and logistic regression performing the worst 
with an AUC of 0.80. Accuracies for all models are roughly comparable, with 
lightgbm still performing the best at 0.91, with the random forest and logistic
regression models only slightly behind at 0.90 each.
